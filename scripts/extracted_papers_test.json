[
  {
    "id": "2509.06956",
    "link": "https://arxiv.org/abs/2509.06956",
    "pdf_link": "https://arxiv.org/pdf/2509.06956",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "title": "H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers",
    "authors": [
      "Wenhao Li",
      "Mengyuan Liu",
      "Hong Liu",
      "Pichao Wang",
      "Shijian Lu",
      "Nicu Sebe"
    ],
    "abstract": "Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at https://github.com/NationalGAILab/HoT.",
    "submission_info": "Submitted 8 September, 2025; originally announced September 2025"
  },
  {
    "id": "2406.06464",
    "link": "https://arxiv.org/abs/2406.06464",
    "pdf_link": "https://arxiv.org/pdf/2406.06464",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "title": "Transforming Wearable Data into Personal Health Insights using Large Language Model Agents",
    "authors": [
      "Mike A. Merrill",
      "Akshay Paruchuri",
      "Naghmeh Rezaei",
      "Geza Kovacs",
      "Javier Perez",
      "Yun Liu",
      "Erik Schenck",
      "Nova Hammerquist",
      "Jake Sunshine",
      "Shyam Tailor",
      "Kumar Ayush",
      "Hao-Wei Su",
      "Qian He",
      "Cory Y. McLean",
      "Mark Malhotra",
      "Shwetak Patel",
      "Jiening Zhan",
      "Tim Althoff",
      "Daniel McDuff",
      "Xin Liu"
    ],
    "abstract": "Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population.",
    "submission_info": "Submitted 8 September, 2025; v1 submitted 10 June, 2024; originally announced June 2024"
  }
]